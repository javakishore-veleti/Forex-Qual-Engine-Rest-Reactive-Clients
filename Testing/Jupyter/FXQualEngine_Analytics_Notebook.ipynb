{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9efe93",
   "metadata": {},
   "source": [
    "# ðŸ“Š FX Qual Engine â€” Complete Analytics Notebook\n",
    "\n",
    "This notebook allows any developer, SRE, QA engineer, or manager to clearly understand **runtime behavior** of the FX Qualification Engine.\n",
    "\n",
    "Included:\n",
    "- ðŸ”¹ API invocation\n",
    "- ðŸ”¹ Real Prometheus metric scraping\n",
    "- ðŸ”¹ Workflow counts (graphs + tables)\n",
    "- ðŸ”¹ Step latency (ms + ns) (graphs + tables)\n",
    "- ðŸ”¹ Load tests to generate more metrics\n",
    "- ðŸ”¹ CSV exports for reports / dashboards\n",
    "\n",
    "All explanations are written in **simple human language**, not data-science jargon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef17cc3",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup â€” Imports & Base URL\n",
    "These imports load all required tools:\n",
    "- `requests` â†’ calls FX Qual Engine\n",
    "- `pandas` â†’ makes tables easy to read\n",
    "- `matplotlib` â†’ graphs\n",
    "- `re` â†’ parse Prometheus metrics\n",
    "- `time` / `os` â†’ load testing & file exports\n",
    "\n",
    "ðŸ’¡ **BASE_URL must match your running FX Qual Engine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277130cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "BASE_URL = \"http://localhost:8080\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcf00e",
   "metadata": {},
   "source": [
    "## â–¶ï¸ API Invocation Helper\n",
    "This function calls your FX Qual Engine once. \n",
    "\n",
    "Each call:\n",
    "- triggers workflow execution\n",
    "- generates Prometheus metrics\n",
    "- returns the quote response\n",
    "\n",
    "This is essential for generating traffic during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a460fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify_fx(client_type=\"rest_client\", amount=1000000):\n",
    "    url = f\"{BASE_URL}/api/fx/qualify?clientType={client_type}\"\n",
    "    payload = {\n",
    "        \"customerId\": \"CUST001\",\n",
    "        \"fromCurrency\": \"USD\",\n",
    "        \"toCurrency\": \"EUR\",\n",
    "        \"quantity\": amount,\n",
    "        \"promoCodes\": [\"FX50\"],\n",
    "        \"productCodes\": [\"FXSPOT\"]\n",
    "    }\n",
    "    resp = requests.post(url, json=payload)\n",
    "    return resp.json(), resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1260f6",
   "metadata": {},
   "source": [
    "## ðŸ“¡ Prometheus Scraper\n",
    "Prometheus exposes metrics in **plain text form**. Example:\n",
    "\n",
    "```\n",
    "fxqual_step_duration_ms_seconds_sum{step=\"customer\"} 0.123\n",
    "```\n",
    "\n",
    "These helpers:\n",
    "- download the raw page\n",
    "- filter lines by metric names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2459cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_prometheus():\n",
    "    return requests.get(f\"{BASE_URL}/actuator/prometheus\").text\n",
    "\n",
    "def find_metrics(raw, key):\n",
    "    return [line for line in raw.split(\"\\n\") if key in line]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d96f18",
   "metadata": {},
   "source": [
    "## ðŸ§® Metric Parsing â†’ DataFrames\n",
    "\n",
    "### Workflow Count\n",
    "`_count` tells us **how many workflows executed**.\n",
    "\n",
    "### Step Latency\n",
    "`_sum Ã· _count` gives average latency in seconds.\n",
    "\n",
    "We convert to:\n",
    "- milliseconds (ms)\n",
    "- nanoseconds (ns)\n",
    "\n",
    "This makes the output readable **even if numbers are tiny**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0c15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_workflow(lines):\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        if \"_count\" not in l:\n",
    "            continue\n",
    "        value = float(l.split()[-1])\n",
    "        m = re.search(r'clientImpl=\"([^\"]+)\"', l)\n",
    "        client = m.group(1) if m else \"unknown\"\n",
    "        data.append({\"clientImpl\": client, \"count\": value})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_step_latency(raw_text):\n",
    "    sum_lines = find_metrics(raw_text, \"fxqual_step_duration_ms_seconds_sum\")\n",
    "    count_lines = find_metrics(raw_text, \"fxqual_step_duration_ms_seconds_count\")\n",
    "\n",
    "    sums = {}\n",
    "    counts = {}\n",
    "\n",
    "    for l in sum_lines:\n",
    "        m = re.search(r'step=\"([^\"]+)\".* ([0-9\\.eE+-]+)$', l)\n",
    "        if m:\n",
    "            sums[m.group(1)] = float(m.group(2))\n",
    "\n",
    "    for l in count_lines:\n",
    "        m = re.search(r'step=\"([^\"]+)\".* ([0-9\\.eE+-]+)$', l)\n",
    "        if m:\n",
    "            counts[m.group(1)] = float(m.group(2))\n",
    "\n",
    "    rows = []\n",
    "    for step in sums:\n",
    "        if step in counts and counts[step] > 0:\n",
    "            avg_sec = (sums[step] / counts[step])\n",
    "            rows.append({\n",
    "                \"step\": step,\n",
    "                \"avg_ms\": avg_sec * 1000,\n",
    "                \"avg_ns\": avg_sec * 1_000_000_000\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe2f68",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Real-Time Workflow Execution Count (Graph)\n",
    "\n",
    "**What this graph shows:**\n",
    "- How many total workflow executions happened\n",
    "- Whether traffic is increasing or idle\n",
    "- Spikes when load testing is running\n",
    "\n",
    "This helps visualize **overall throughput** of your FX engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1140e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_workflow(i):\n",
    "    raw = fetch_prometheus()\n",
    "    wf = find_metrics(raw, \"fxqual_workflow_duration_ms_seconds_count\")\n",
    "    df = parse_workflow(wf)\n",
    "    plt.cla()\n",
    "    if not df.empty:\n",
    "        plt.plot(df[\"count\"], marker=\"o\")\n",
    "    plt.title(\"Real-Time Workflow Execution Count\")\n",
    "    plt.xlabel(\"Client Type Index\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ani = animation.FuncAnimation(fig, animate_workflow, interval=2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d7e88",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step Latency (Graph + Table)\n",
    "\n",
    "**This block explains exactly how fast each downstream integration is.**\n",
    "\n",
    "Real interpretation for developers:\n",
    "\n",
    "- `customer` â†’ Slowest, because it simulates a DB/API lookup\n",
    "- `promo` â†’ Lightweight business rule operation\n",
    "- `product` â†’ Very fast integration\n",
    "- `fxInterest` â†’ Super fast microservice or static rate source\n",
    "\n",
    "The table includes **both ms and ns**, which helps when values are tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4dc78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = fetch_prometheus()\n",
    "steps_df = parse_step_latency(raw)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "if not steps_df.empty:\n",
    "    plt.bar(steps_df[\"step\"], steps_df[\"avg_ms\"])\n",
    "plt.title(\"Average Step Latency (ms)\")\n",
    "plt.ylabel(\"Milliseconds\")\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()\n",
    "\n",
    "steps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167452d",
   "metadata": {},
   "source": [
    "## ðŸš€ Load Test Helper\n",
    "Creates traffic so metrics populate correctly.\n",
    "\n",
    "Great for demos, QA testing, or verifying your Prometheus setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc098f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(n=20, client_type=\"rest_client\"):\n",
    "    for i in range(n):\n",
    "        qualify_fx(client_type)\n",
    "        time.sleep(0.1)\n",
    "    return f\"Load test complete: {n} calls\"\n",
    "\n",
    "load_test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57e3d6",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Export CSV Files\n",
    "Produces CSVs that can be shared with teams:\n",
    "- QA\n",
    "- SRE\n",
    "- Architecture\n",
    "- Product managers\n",
    "\n",
    "Very helpful for offline analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ec189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = fetch_prometheus()\n",
    "workflow_df = parse_workflow(find_metrics(raw, \"fxqual_workflow_duration_ms_seconds_count\"))\n",
    "steps_df = parse_step_latency(raw)\n",
    "\n",
    "os.makedirs(\"exports\", exist_ok=True)\n",
    "workflow_df.to_csv(\"exports/workflow_metrics.csv\", index=False)\n",
    "steps_df.to_csv(\"exports/step_metrics.csv\", index=False)\n",
    "\n",
    "workflow_df, steps_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
